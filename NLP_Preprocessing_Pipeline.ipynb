{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55dd7097-342a-4c71-a4a1-ae16459b1d1f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In any NLP task, cleaning or preprocessing the dataset is as important as model building.\n",
    "Some of the common preprocessing steps are given below with example.\n",
    "In this notebook I will do preprocessing using both NLTK and SPACY and also will make a comparison between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e4a059-7550-4c00-9203-05439cf40621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessing Complete ---\n",
      "                           text        processed_text\n",
      "0     I want to return my order     want return order\n",
      "1     My charger is not working      charger not work\n",
      "2     The delivery is very late    deliveri veri late\n",
      "3  I received a damaged product  receiv damag product\n",
      "4               I need a refund           need refund\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "STOP_WORDS = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
    "    'he', 'him', 'his', 'she', 'her', 'it', 'its', 'they', 'them', 'their', \n",
    "    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'a', 'an', 'the', 'and', \n",
    "    'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', \n",
    "    'for', 'with', 'about', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off'}\n",
    "\n",
    "def preprocessing_pipeline(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Splitting the text into tokens\n",
    "    tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    tokens = [item.strip() for item in tokens if item.strip()]\n",
    "    \n",
    "    # Stop Word & Punctuation Removal\n",
    "    clean_tokens = [t for t in tokens if t not in STOP_WORDS and t not in '.,:;?!\"()\\'-_']\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
    "    \n",
    "    return \" \".join(stemmed_tokens)\n",
    "\n",
    "df = pd.read_csv('customer_support_dataset.csv')\n",
    "df['processed_text'] = df['text'].apply(preprocessing_pipeline)\n",
    "\n",
    "# 3. Final Output\n",
    "print(\"--- Preprocessing Complete ---\")\n",
    "print(df[['text', 'processed_text']].head())\n",
    "\n",
    "# Download the preprocessed csv file\n",
    "df.to_csv('preprocessed_customer_support.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99645d29-5c7a-4b28-a68f-03b32d5c20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- spaCy Preprocessing Complete ---\n",
      "                           text    processed_text_spacy\n",
      "0     I want to return my order       want return order\n",
      "1     My charger is not working            charger work\n",
      "2     The delivery is very late           delivery late\n",
      "3  I received a damaged product  receive damage product\n",
      "4               I need a refund             need refund\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# 1. Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_preprocessing_pipeline(text):\n",
    "    # Normalization: Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Cleaning & Lemmatization:\n",
    "    # Filter out Stop Words and Punctuation \n",
    "    clean_lemmas = [\n",
    "        token.lemma_            # Get the dictionary root \n",
    "        for token in doc \n",
    "        if not token.is_stop    # Remove common stop words\n",
    "        and not token.is_punct  # Remove punctuation\n",
    "        and not token.is_space  # Remove extra whitespace\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(clean_lemmas)\n",
    "\n",
    "# Execution\n",
    "df = pd.read_csv('customer_support_dataset.csv')\n",
    "df['processed_text_spacy'] = df['text'].apply(spacy_preprocessing_pipeline)\n",
    "\n",
    "# Final Output\n",
    "print(\"--- spaCy Preprocessing Complete ---\")\n",
    "print(df[['text', 'processed_text_spacy']].head())\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "df.to_csv('spacy_preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f84d9e-aaee-41de-b064-851324e58f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
